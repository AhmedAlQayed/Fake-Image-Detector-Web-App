{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1380, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Created By Ahmed Al Hammadi under supoervision of Mr. Anshu Pandey during an AI Training Course\n",
    "# Date: 22 - April - 2019\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "#################### Data Preprocessing ###########################################\n",
    "\n",
    "\n",
    "# Simple Function to load an image using open CV library\n",
    "def load_image(path):\n",
    "    img= cv2.imread(path)\n",
    "    return img\n",
    "  \n",
    "# a function to randomly crop an image with a specific width and height\n",
    "def randomCrop(img, width, height):\n",
    "    assert img.shape[0] >= height\n",
    "    assert img.shape[1] >= width\n",
    "    x = random.randint(0, img.shape[1] - width)\n",
    "    y = random.randint(0, img.shape[0] - height)\n",
    "    img = img[y:y+height, x:x+width]\n",
    "    return img\n",
    "\n",
    "# a function to locate the corresponding mask of the fake image and crop it with some extra margin \n",
    "def mask_crop(path):\n",
    "    #Read and resize the image\n",
    "    img = cv2.imread(path) \n",
    "    img = cv2.resize(img,(299,299))\n",
    "    # Converts images from BGR to HSV\n",
    "    hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "    #Grayscale range for detection\n",
    "    black_lower = np.array([0,0,0],np.uint8)\n",
    "    black_upper = np.array([25,25,25],np.uint8)\n",
    "    black = cv2.inRange(hsv, black_lower, black_upper)\n",
    "\n",
    "    # Morphological Transform, Dilation\n",
    "    kernal = np.ones((5, 5), \"uint8\")\n",
    "\n",
    "    # The bitwise and of the frame and mask is done so\n",
    "    # that only the red coloured objects are highlighted\n",
    "    # and stored in res\n",
    "    black = cv2.dilate(black, kernal)\n",
    "    # detect the mask using fincCountours and extract the dimensions of the bounding bax to be using for cropping\n",
    "    (_,contours, hierarchy)=cv2.findContours(black, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, contour in enumerate(contours):\n",
    "        area = cv2.contourArea(contour)\n",
    "        if(area > 50): #if detected area > 50 pixels proceed!\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            w2=math.ceil(0.01*w) # Take only 1% more margin\n",
    "            h2=math.ceil(0.01*h)\n",
    "            x2=x-math.ceil(0.05*x) # Recenter the bounding box\n",
    "            y2=y-math.ceil(0.05*y)\n",
    "        else:\n",
    "            y2=None # to handle an error when there is no detection\n",
    "    img2 = cv2.imread(path[:-9]+\".jpg\") # read the corresponding image to the mask\n",
    "    img2 = cv2.resize(img2,(299,299))\n",
    "    if y2:\n",
    "        img = img2[y2:y + h + h2, x2:x + w + w2] # if bounding box exists, crop the fake portion\n",
    "        img = cv2.resize(img,(299,299)) # Resize according to InceptionV3  requirements\n",
    "        return img\n",
    "    else:\n",
    "        return None\n",
    "#\n",
    "folders=['fake','pristine'] # Define folders in directory\n",
    "trainimg=[] # empty list for training data\n",
    "trainlabel=[] # empty list for training labels\n",
    "for folder in folders: # iterate in each folder \n",
    "    direc = \"C:\\\\Users\\\\CUDA1\\\\Downloads\\\\training\"+\"\\\\\"+folder\n",
    "    images = os.listdir(direc)\n",
    "    for path in images:\n",
    "        if folder=='fake':\n",
    "            if \"mask\" in path: # if mask is included in filename, apply mask crop\n",
    "                img = mask_crop(direc+'/'+path)\n",
    "                if img is not None:\n",
    "                    trainimg.append(img)\n",
    "                    trainlabel.append(folder)\n",
    "        elif folder=='pristine':\n",
    "            img = load_image(direc+'/'+path) # Otherwise, apply a similar random crop to the pristine image\n",
    "            img = randomCrop(img,299,299)\n",
    "            trainimg.append(img)\n",
    "            trainlabel.append(folder)\n",
    "            \n",
    "trainimg = np.concatenate([trainimg])\n",
    "trainlabel = np.array(trainlabel).reshape(-1,1)\n",
    "trainlabel = OneHotEncoder().fit_transform(trainlabel).toarray()\n",
    "trainlabel.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CUDA1\\Anaconda3\\envs\\training\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\CUDA1\\Anaconda3\\envs\\training\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CUDA1\\Anaconda3\\envs\\training\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1116 samples, validate on 280 samples\n",
      "Epoch 1/10\n",
      "1116/1116 [==============================] - 10s 9ms/step - loss: 5.2439 - acc: 0.6738 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "1116/1116 [==============================] - 4s 4ms/step - loss: 4.9972 - acc: 0.6900 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "1116/1116 [==============================] - 4s 4ms/step - loss: 4.9972 - acc: 0.6900 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "1116/1116 [==============================] - 4s 4ms/step - loss: 4.9972 - acc: 0.6900 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "1116/1116 [==============================] - 4s 4ms/step - loss: 4.9972 - acc: 0.6900 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "1116/1116 [==============================] - 4s 4ms/step - loss: 4.9972 - acc: 0.6900 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "1116/1116 [==============================] - 4s 4ms/step - loss: 4.9972 - acc: 0.6900 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "1116/1116 [==============================] - 4s 4ms/step - loss: 4.9972 - acc: 0.6900 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "1116/1116 [==============================] - 4s 4ms/step - loss: 4.9972 - acc: 0.6900 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "1116/1116 [==============================] - 4s 4ms/step - loss: 4.9972 - acc: 0.6900 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c84e50438>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Training a model to Learn-from-screach  ###########################################\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=(299,299,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(28, kernel_size=(3,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2,activation=tf.nn.softmax))\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x=trainimg,y=trainlabel,epochs=10,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CUDA1\\Downloads\n"
     ]
    }
   ],
   "source": [
    "#For Save the Model\n",
    "model.save('keras_model_from_scratch.hdf5') \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 924 samples, validate on 456 samples\n",
      "Epoch 1/35\n",
      "924/924 [==============================] - 49s 53ms/step - loss: 0.8271 - acc: 0.4670 - val_loss: 0.4947 - val_acc: 0.8037\n",
      "Epoch 2/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.6944 - acc: 0.5860 - val_loss: 0.4195 - val_acc: 0.8969\n",
      "Epoch 3/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.6171 - acc: 0.6580 - val_loss: 0.3868 - val_acc: 0.9189\n",
      "Epoch 4/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.5637 - acc: 0.7181 - val_loss: 0.3596 - val_acc: 0.9397\n",
      "Epoch 5/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.5219 - acc: 0.7478 - val_loss: 0.3437 - val_acc: 0.9474\n",
      "Epoch 6/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.4854 - acc: 0.7868 - val_loss: 0.3344 - val_acc: 0.9463\n",
      "Epoch 7/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.4514 - acc: 0.8187 - val_loss: 0.3249 - val_acc: 0.9496\n",
      "Epoch 8/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.4294 - acc: 0.8360 - val_loss: 0.3143 - val_acc: 0.9529\n",
      "Epoch 9/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.4046 - acc: 0.8506 - val_loss: 0.3067 - val_acc: 0.9550\n",
      "Epoch 10/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.3878 - acc: 0.8561 - val_loss: 0.2992 - val_acc: 0.9550\n",
      "Epoch 11/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.3666 - acc: 0.8734 - val_loss: 0.2931 - val_acc: 0.9561\n",
      "Epoch 12/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.3496 - acc: 0.8788 - val_loss: 0.2858 - val_acc: 0.9572\n",
      "Epoch 13/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.3412 - acc: 0.8880 - val_loss: 0.2812 - val_acc: 0.9583\n",
      "Epoch 14/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.3251 - acc: 0.8934 - val_loss: 0.2766 - val_acc: 0.9572\n",
      "Epoch 15/35\n",
      "924/924 [==============================] - 14s 15ms/step - loss: 0.3135 - acc: 0.8977 - val_loss: 0.2718 - val_acc: 0.9583\n",
      "Epoch 16/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.3048 - acc: 0.9058 - val_loss: 0.2684 - val_acc: 0.9594\n",
      "Epoch 17/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2939 - acc: 0.9096 - val_loss: 0.2683 - val_acc: 0.9561\n",
      "Epoch 18/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2833 - acc: 0.9091 - val_loss: 0.2640 - val_acc: 0.9572\n",
      "Epoch 19/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2836 - acc: 0.9172 - val_loss: 0.2605 - val_acc: 0.9594\n",
      "Epoch 20/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2702 - acc: 0.9188 - val_loss: 0.2582 - val_acc: 0.9572\n",
      "Epoch 21/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2654 - acc: 0.9205 - val_loss: 0.2553 - val_acc: 0.9583\n",
      "Epoch 22/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2617 - acc: 0.9264 - val_loss: 0.2541 - val_acc: 0.9594\n",
      "Epoch 23/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2571 - acc: 0.9269 - val_loss: 0.2529 - val_acc: 0.9583\n",
      "Epoch 24/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2442 - acc: 0.9313 - val_loss: 0.2503 - val_acc: 0.9594\n",
      "Epoch 25/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2483 - acc: 0.9286 - val_loss: 0.2494 - val_acc: 0.9594\n",
      "Epoch 26/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2411 - acc: 0.9264 - val_loss: 0.2473 - val_acc: 0.9605\n",
      "Epoch 27/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2286 - acc: 0.9356 - val_loss: 0.2456 - val_acc: 0.9605\n",
      "Epoch 28/35\n",
      "924/924 [==============================] - 15s 16ms/step - loss: 0.2304 - acc: 0.9432 - val_loss: 0.2441 - val_acc: 0.9605\n",
      "Epoch 29/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2243 - acc: 0.9372 - val_loss: 0.2415 - val_acc: 0.9605\n",
      "Epoch 30/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2190 - acc: 0.9383 - val_loss: 0.2412 - val_acc: 0.9594\n",
      "Epoch 31/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2216 - acc: 0.9394 - val_loss: 0.2404 - val_acc: 0.9594\n",
      "Epoch 32/35\n",
      "924/924 [==============================] - 14s 16ms/step - loss: 0.2140 - acc: 0.9399 - val_loss: 0.2391 - val_acc: 0.9594\n",
      "Epoch 33/35\n",
      "924/924 [==============================] - 15s 16ms/step - loss: 0.2241 - acc: 0.9329 - val_loss: 0.2397 - val_acc: 0.9572\n",
      "Epoch 34/35\n",
      "924/924 [==============================] - 15s 16ms/step - loss: 0.2189 - acc: 0.9329 - val_loss: 0.2401 - val_acc: 0.9561\n",
      "Epoch 35/35\n",
      "924/924 [==============================] - 15s 16ms/step - loss: 0.2014 - acc: 0.9448 - val_loss: 0.2379 - val_acc: 0.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20f9fff3e80>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Training a model using Transfer Learning ###########################################\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input,decode_predictions\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=0.001)\n",
    "\n",
    "CLASSES = 2 #Fake or Pristine - Two Classes\n",
    "\n",
    "# Load the model without the top layer which will be trained using our data\n",
    "base_model = ResNet50(weights='imagenet', include_top=False) \n",
    "\n",
    "x = base_model.output\n",
    "\n",
    "x = GlobalAveragePooling2D(name='avg_pool2')(x)\n",
    "\n",
    "predictions = Dense(CLASSES, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "   \n",
    "#transfer learning\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=trainimg,y=trainlabel,epochs=35,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real\n",
      "fake\n",
      "fake\n",
      "fake\n",
      "fake\n",
      "fake\n",
      "fake\n",
      "real\n",
      "real\n",
      "fake\n",
      "real\n",
      "fake\n",
      "real\n",
      "fake\n",
      "real\n",
      "fake\n",
      "real\n",
      "fake\n",
      "real\n",
      "fake\n",
      "real\n",
      "fake\n",
      "fake\n",
      "fake\n"
     ]
    }
   ],
   "source": [
    "##################### Testing the model before deploying #############################\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from sklearn.feature_extraction import image as ims\n",
    "def preprocess_input(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x *= 2.\n",
    "    return x\n",
    "#img_path = \"C:\\\\Users\\\\CUDA1\\\\Downloads\\\\phoroa\\\\windsor.jpg\"\n",
    "#img_path = \"C:\\\\Users\\\\CUDA1\\\\Downloads\\\\phoroa\\\\fake_putin.jpg\"\n",
    "#img_path = \"C:\\\\Users\\\\CUDA1\\\\Downloads\\\\phoroa\\\\real_putin.jpg\"\n",
    "#img_path = \"C:\\\\Users\\\\CUDA1\\\\Downloads\\\\phoroa\\\\Bush_fake.jpg\"\n",
    "#img_path = \"C:\\\\Users\\\\CUDA1\\\\Downloads\\\\phoroa\\\\fake_tourist.jpg\"\n",
    "\n",
    "img =cv2.imread(img_path)\n",
    "img = cv2.resize(img,(350,350))\n",
    "nPatches=math.floor(img.size/(224*224))\n",
    "patches = ims.extract_patches_2d(img, (224, 224))\n",
    "threshold=0.1\n",
    "predictions=[]\n",
    "i=0\n",
    "while (i<len(patches)):\n",
    "    x = image.img_to_array(patches[i])\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    preds = model.predict(x)\n",
    "    predictions.append(str(np.argmax(preds)))\n",
    "    i=i+ (nPatches * 100)\n",
    "s=0\n",
    "j=0\n",
    "m=0\n",
    "while (s<len(predictions)):\n",
    "    if (predictions[s])=='0':\n",
    "        j=j+1\n",
    "        print(\"real\")\n",
    "    if (predictions[s])=='1':\n",
    "        print(\"fake\")\n",
    "        m=m+1\n",
    "    s=s+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1104 samples, validate on 276 samples\n",
      "Epoch 1/10\n",
      "1104/1104 [==============================] - 52s 47ms/step - loss: 0.7066 - acc: 0.5743 - val_loss: 0.5267 - val_acc: 0.7464\n",
      "Epoch 2/10\n",
      "1104/1104 [==============================] - 14s 13ms/step - loss: 0.5913 - acc: 0.6984 - val_loss: 0.4501 - val_acc: 0.8297\n",
      "Epoch 3/10\n",
      "1104/1104 [==============================] - 14s 13ms/step - loss: 0.5289 - acc: 0.7360 - val_loss: 0.4115 - val_acc: 0.8605\n",
      "Epoch 4/10\n",
      "1104/1104 [==============================] - 14s 13ms/step - loss: 0.4917 - acc: 0.7704 - val_loss: 0.3885 - val_acc: 0.8931\n",
      "Epoch 5/10\n",
      "1104/1104 [==============================] - 14s 13ms/step - loss: 0.4461 - acc: 0.7994 - val_loss: 0.3699 - val_acc: 0.9076\n",
      "Epoch 6/10\n",
      "1104/1104 [==============================] - 14s 13ms/step - loss: 0.4122 - acc: 0.8270 - val_loss: 0.3477 - val_acc: 0.9203\n",
      "Epoch 7/10\n",
      "1104/1104 [==============================] - 14s 13ms/step - loss: 0.3854 - acc: 0.8533 - val_loss: 0.3316 - val_acc: 0.9275\n",
      "Epoch 8/10\n",
      "1104/1104 [==============================] - 14s 13ms/step - loss: 0.3618 - acc: 0.8619 - val_loss: 0.3195 - val_acc: 0.9275\n",
      "Epoch 9/10\n",
      "1104/1104 [==============================] - 14s 13ms/step - loss: 0.3407 - acc: 0.8727 - val_loss: 0.3116 - val_acc: 0.9312\n",
      "Epoch 10/10\n",
      "1104/1104 [==============================] - 14s 13ms/step - loss: 0.3279 - acc: 0.8886 - val_loss: 0.2987 - val_acc: 0.9348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20fb0e5a8d0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Training a model using Transfer Learning ###########################################\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input,decode_predictions\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=0.001)\n",
    "\n",
    "CLASSES = 2 #Fake or Pristine - Two Classes\n",
    "\n",
    "# Load the model without the top layer which will be trained using our data\n",
    "base_model = ResNet50(weights='imagenet', include_top=False) \n",
    "\n",
    "x = base_model.output\n",
    "\n",
    "x = GlobalAveragePooling2D(name='avg_pool2')(x)\n",
    "\n",
    "predictions = Dense(CLASSES, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "   \n",
    "#transfer learning\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=trainimg,y=trainlabel,epochs=10,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"C:\\\\Users\\\\CUDA1\\\\Downloads\\\\keras_model4.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
